name: üöÄ Smart Edu Backend CI/CD Pipeline

# Trigger workflow on push to main/staging/develop branches, or manual trigger
on:
  push:
    branches: [ main, staging, develop ]  # Auto-deploy on push to these branches
  pull_request:
    branches: [ main, staging, develop ]  # Run checks on PRs (no deployment)
  workflow_dispatch:  # Allow manual trigger from GitHub Actions UI
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
          - development

# Global environment variables used across all jobs
env:
  NODE_VERSION: '20'
  REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ secrets.AWS_REGION }}.amazonaws.com  # ECR URL constructed from secrets
  IMAGE_NAME: smart-edu  # Base name for Docker images
  AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}
  ECR_IMAGE: ${{ vars.ECR_IMAGE }}
  ECS_CLUSTER: ${{ vars.ECS_CLUSTER }}

jobs:

  # üê≥ STEP 1: Build Docker image and push to ECR (runs for ALL branches)
  docker-build:
    name: üê≥ Docker Build & Security
    runs-on: ubuntu-latest
    # needs: quality-check  # Uncomment if you add tests/linting first
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: üê≥ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3  # Enable multi-platform builds

      - name: üîê Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: üè∑Ô∏è Extract metadata (creates image tags based on branch/PR)
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch           # Tag: staging, main, develop
            type=ref,event=pr               # Tag: pr-42
            type=sha,prefix={{branch}}-     # Tag: staging-abc1234
            type=raw,value=latest,enable={{is_default_branch}}  # Tag: latest (only for main)

      - name: üèóÔ∏è Build Docker image (AMD64) - test build only
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false  # Don't push yet, just verify it builds
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha  # Use GitHub Actions cache for faster builds
          cache-to: type=gha,mode=max
          platforms: linux/amd64  # Intel/AMD processors

      - name: üèóÔ∏è Build Docker image (ARM64) - test build only
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false  # Don't push yet, just verify it builds
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/arm64  # ARM processors (e.g., AWS Graviton)

      - name: üì§ Push Docker images to ECR (both AMD64 + ARM64)
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true  # Now push to ECR after successful test builds
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64  # Multi-architecture image

  # üöÄ STEP 2: Deploy to Staging (only runs for staging/develop branches)
  deploy-staging:
    name: üöÄ Deploy to Staging
    runs-on: ubuntu-latest
    needs: [docker-build]  # Wait for docker-build to complete first
    if: github.ref == 'refs/heads/staging' || github.event_name == 'workflow_dispatch' || github.ref == 'refs/heads/develop'  # Only deploy staging/develop branches
    environment: |
      staging
      development
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: üîê Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: üê≥ Deploy to staging ECS
        run: |
          echo "üöÄ Deploying to staging environment..."
          # Check if staging cluster/service secrets are set
          if [ -n "${{ secrets.AWS_STAGING_CLUSTER }}" ] && [ -n "${{ secrets.AWS_STAGING_SERVICE }}" ]; then
            echo "üìù Updating ECS task definition with staging environment variables..."
            
            # Step 1: Get current task definition and remove read-only fields
            TASK_DEFINITION=$(aws ecs describe-task-definition --task-definition smart-edu-task --query 'taskDefinition' | jq 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy)')
            
            # Step 2: Inject all environment variables from GitHub secrets into task definition
            UPDATED_TASK_DEF=$(echo "$TASK_DEFINITION" | jq --arg NODE_ENV "${{ secrets.NODE_ENV_STAGING || 'staging' }}" \
              --arg DATABASE_URL "${{ secrets.DATABASE_URL_STAGING }}" \
              --arg JWT_SECRET "${{ secrets.JWT_SECRET }}" \
              --arg EMAIL_USER "${{ secrets.EMAIL_USER }}" \
              --arg EMAIL_PASSWORD "${{ secrets.EMAIL_PASSWORD }}" \
              --arg OPENAI_API_KEY "${{ secrets.OPENAI_API_KEY }}" \
              --arg AWS_ACCESS_KEY_ID "${{ secrets.AWS_ACCESS_KEY_ID }}" \
              --arg AWS_SECRET_ACCESS_KEY "${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
              --arg AWS_REGION "${{ secrets.AWS_REGION }}" \
              --arg AWS_S3_BUCKET "${{ secrets.AWS_S3_BUCKET }}" \
              --arg PINECONE_API_KEY "${{ secrets.PINECONE_API_KEY }}" \
              --arg PINECONE_ENVIRONMENT "${{ secrets.PINECONE_ENVIRONMENT }}" \
              --arg GOOGLE_SMTP_HOST "${{ secrets.GOOGLE_SMTP_HOST }}" \
              --arg GOOGLE_SMTP_PORT "${{ secrets.GOOGLE_SMTP_PORT }}" \
              --arg FRONTEND_URL "${{ secrets.FRONTEND_URL }}" \
              --arg CORS_ORIGINS "${{ secrets.CORS_ORIGINS }}" \
              --arg APP_NAME "${{ secrets.APP_NAME }}" \
              --arg BTECH_ADMIN_EMAIL "${{ secrets.BTECH_ADMIN_EMAIL }}" \
              --arg OTP_EXPIRES_AT "${{ secrets.OTP_EXPIRES_AT }}" \
              '.containerDefinitions[0].environment = [
                {"name": "NODE_ENV", "value": $NODE_ENV},
                {"name": "PORT", "value": "3000"},  # App listens on port 3000 inside container
                {"name": "DATABASE_URL", "value": $DATABASE_URL},  # Neon staging DB URL
                {"name": "JWT_SECRET", "value": $JWT_SECRET},
                {"name": "EMAIL_USER", "value": $EMAIL_USER},
                {"name": "EMAIL_PASSWORD", "value": $EMAIL_PASSWORD},
                {"name": "OPENAI_API_KEY", "value": $OPENAI_API_KEY},
                {"name": "AWS_ACCESS_KEY_ID", "value": $AWS_ACCESS_KEY_ID},
                {"name": "AWS_SECRET_ACCESS_KEY", "value": $AWS_SECRET_ACCESS_KEY},
                {"name": "AWS_REGION", "value": $AWS_REGION},
                {"name": "AWS_S3_BUCKET", "value": $AWS_S3_BUCKET},
                {"name": "PINECONE_API_KEY", "value": $PINECONE_API_KEY},
                {"name": "PINECONE_ENVIRONMENT", "value": $PINECONE_ENVIRONMENT},
                {"name": "GOOGLE_SMTP_HOST", "value": $GOOGLE_SMTP_HOST},
                {"name": "GOOGLE_SMTP_PORT", "value": $GOOGLE_SMTP_PORT},
                {"name": "FRONTEND_URL", "value": $FRONTEND_URL},
                {"name": "CORS_ORIGINS", "value": $CORS_ORIGINS},
                {"name": "APP_NAME", "value": $APP_NAME},
                {"name": "BTECH_ADMIN_EMAIL", "value": $BTECH_ADMIN_EMAIL},
                {"name": "OTP_EXPIRES_AT", "value": $OTP_EXPIRES_AT}
              ]')
            
            # Step 3: Register new task definition revision with AWS
            NEW_TASK_DEF=$(aws ecs register-task-definition --cli-input-json "$UPDATED_TASK_DEF" --query 'taskDefinition.taskDefinitionArn' --output text)
            
            # Step 4: Update ECS service to use new task definition (triggers rolling deployment)
            aws ecs update-service --cluster "${{ secrets.AWS_STAGING_CLUSTER }}" --service "${{ secrets.AWS_STAGING_SERVICE }}" --task-definition "$NEW_TASK_DEF" --force-new-deployment
          else
            echo "‚ö†Ô∏è AWS staging cluster/service secrets not set - skipping AWS deployment"
            echo "To enable AWS deployment, add AWS_STAGING_CLUSTER and AWS_STAGING_SERVICE secrets"
          fi
          
          # For Docker Compose (alternative)
          # docker-compose -f docker-compose.staging.yml up -d --build

      - name: üß™ Run smoke tests (basic health checks)
        run: |
          echo "üß™ Running smoke tests..."
          sleep 30  # Wait for ECS to start new tasks
          
          # Test health endpoints if STAGING_URL is provided
          if [ -n "${{ secrets.STAGING_URL }}" ]; then
            echo "Testing staging URL: ${{ secrets.STAGING_URL }}"
            curl -f "${{ secrets.STAGING_URL }}/health" || echo "‚ö†Ô∏è Health check failed - normal if URL not configured"
            curl -f "${{ secrets.STAGING_URL }}/api/v1/health" || echo "‚ö†Ô∏è API health failed - normal if URL not configured"
          else
            echo "‚ö†Ô∏è STAGING_URL secret not set - skipping smoke tests"
            echo "Add STAGING_URL secret to enable automated health checks"
          fi

      # - name: üìä Notify deployment status
      #   uses: 8398a7/action-slack@v3
      #   if: always()
      #   with:
      #     status: ${{ job.status }}
      #     channel: '#deployments'
      #     webhook_url: ${{ secrets.SLACK_WEBHOOK }}
      #   env:
      #     SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # üöÄ STEP 3: Deploy to Production (only runs for main branch)
  deploy-production:
    name: üöÄ Deploy to Production
    runs-on: ubuntu-latest
    needs: [docker-build]  # Wait for docker-build to complete first
    if: github.ref == 'refs/heads/main' || (github.event_name == 'workflow_dispatch' && github.event.inputs.environment == 'production')  # Only deploy main branch
    environment: production  # Requires manual approval if configured in GitHub
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîê Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: üîê Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: üîç Verify ECS Resources (pre-deployment check)
        run: |
          echo "üîç Verifying production ECS cluster and service exist..."
          aws ecs describe-clusters --clusters "${{ secrets.AWS_PRODUCTION_CLUSTER }}" --query 'clusters[0].status'
          aws ecs describe-services --cluster "${{ secrets.AWS_PRODUCTION_CLUSTER }}" --services "${{ secrets.AWS_PRODUCTION_SERVICE }}" --query 'services[0].status'

      - name: üìã Get Current Task Definition
        run: |
          echo "üìã Retrieving current production task definition..."
          aws ecs describe-task-definition --task-definition smart-edu-task --query 'taskDefinition' > current-task-def.json
          echo "‚úÖ Task definition retrieved"

      - name: üîÑ Update Task Definition with Production Env Vars
        run: |
          echo "üîÑ Updating task definition with production environment variables..."
          # Step 1: Get current task definition and remove read-only fields
          aws ecs describe-task-definition --task-definition smart-edu-task --query 'taskDefinition' | jq 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .compatibilities, .registeredAt, .registeredBy)' > task-def.json
          
          # Step 2: Inject production environment variables from GitHub secrets
          jq --arg NODE_ENV "${{ secrets.NODE_ENV_PRODUCTION || 'production' }}" \
            --arg DATABASE_URL "${{ secrets.DATABASE_URL_PRODUCTION }}" \
            --arg JWT_SECRET "${{ secrets.JWT_SECRET }}" \
            --arg EMAIL_USER "${{ secrets.EMAIL_USER }}" \
            --arg EMAIL_PASSWORD "${{ secrets.EMAIL_PASSWORD }}" \
            --arg OPENAI_API_KEY "${{ secrets.OPENAI_API_KEY }}" \
            --arg AWS_ACCESS_KEY_ID "${{ secrets.AWS_ACCESS_KEY_ID }}" \
            --arg AWS_SECRET_ACCESS_KEY "${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            --arg AWS_REGION "${{ secrets.AWS_REGION }}" \
            --arg AWS_S3_BUCKET "${{ secrets.AWS_S3_BUCKET }}" \
            --arg PINECONE_API_KEY "${{ secrets.PINECONE_API_KEY }}" \
            --arg PINECONE_ENVIRONMENT "${{ secrets.PINECONE_ENVIRONMENT }}" \
            --arg GOOGLE_SMTP_HOST "${{ secrets.GOOGLE_SMTP_HOST }}" \
            --arg GOOGLE_SMTP_PORT "${{ secrets.GOOGLE_SMTP_PORT }}" \
            --arg FRONTEND_URL "${{ secrets.FRONTEND_URL }}" \
            --arg CORS_ORIGINS "${{ secrets.CORS_ORIGINS }}" \
            --arg APP_NAME "${{ secrets.APP_NAME }}" \
            --arg BTECH_ADMIN_EMAIL "${{ secrets.BTECH_ADMIN_EMAIL }}" \
            --arg OTP_EXPIRES_AT "${{ secrets.OTP_EXPIRES_AT }}" \
            '.containerDefinitions[0].environment = [
              {"name": "NODE_ENV", "value": $NODE_ENV},
              {"name": "PORT", "value": "3000"},  # App listens on port 3000 inside container
              {"name": "DATABASE_URL", "value": $DATABASE_URL},  # Neon production DB URL
              {"name": "JWT_SECRET", "value": $JWT_SECRET},
              {"name": "EMAIL_USER", "value": $EMAIL_USER},
              {"name": "EMAIL_PASSWORD", "value": $EMAIL_PASSWORD},
              {"name": "OPENAI_API_KEY", "value": $OPENAI_API_KEY},
              {"name": "AWS_ACCESS_KEY_ID", "value": $AWS_ACCESS_KEY_ID},
              {"name": "AWS_SECRET_ACCESS_KEY", "value": $AWS_SECRET_ACCESS_KEY},
              {"name": "AWS_REGION", "value": $AWS_REGION},
              {"name": "AWS_S3_BUCKET", "value": $AWS_S3_BUCKET},
              {"name": "PINECONE_API_KEY", "value": $PINECONE_API_KEY},
              {"name": "PINECONE_ENVIRONMENT", "value": $PINECONE_ENVIRONMENT},
              {"name": "GOOGLE_SMTP_HOST", "value": $GOOGLE_SMTP_HOST},
              {"name": "GOOGLE_SMTP_PORT", "value": $GOOGLE_SMTP_PORT},
              {"name": "FRONTEND_URL", "value": $FRONTEND_URL},
              {"name": "CORS_ORIGINS", "value": $CORS_ORIGINS},
              {"name": "APP_NAME", "value": $APP_NAME},
              {"name": "BTECH_ADMIN_EMAIL", "value": $BTECH_ADMIN_EMAIL},
              {"name": "OTP_EXPIRES_AT", "value": $OTP_EXPIRES_AT}
            ]' task-def.json > updated-task-def.json
          
          echo "‚úÖ Task definition updated successfully"
            
      - name: üìù Register New Task Definition with AWS
        run: |
          echo "üìù Registering new task definition revision..."
          NEW_TASK_DEF=$(aws ecs register-task-definition --cli-input-json file://updated-task-def.json --query 'taskDefinition.taskDefinitionArn' --output text)
          echo "‚úÖ New task definition registered: $NEW_TASK_DEF"

      - name: üöÄ Deploy to Production ECS Service
        run: |
          echo "üöÄ Updating production ECS service with new task definition..."
          aws ecs update-service --cluster "${{ secrets.AWS_PRODUCTION_CLUSTER }}" --service "${{ secrets.AWS_PRODUCTION_SERVICE }}" --task-definition "$NEW_TASK_DEF" --force-new-deployment
          echo "‚úÖ Service update initiated - rolling deployment started"

      - name: ‚è≥ Wait for Deployment to Complete
        run: |
          echo "‚è≥ Waiting for production deployment to stabilize (this may take several minutes)..."
          aws ecs wait services-stable --cluster "${{ secrets.AWS_PRODUCTION_CLUSTER }}" --services "${{ secrets.AWS_PRODUCTION_SERVICE }}"
          echo "‚úÖ Production deployment completed successfully!"
          else
            echo "‚ö†Ô∏è AWS production cluster/service secrets not set - skipping AWS deployment"
            echo "To enable AWS deployment, add AWS_PRODUCTION_CLUSTER and AWS_PRODUCTION_SERVICE secrets"
          fi
          
          # For Docker Compose (alternative)
          # docker-compose -f docker-compose.prod.yml up -d --build

      - name: üß™ Run Production Smoke Tests
        run: |
          echo "üß™ Running production smoke tests..."
          sleep 30  # Wait for ECS to start new tasks
          
          # Test health endpoints and SSL if PRODUCTION_URL is provided
          if [ -n "${{ secrets.PRODUCTION_URL }}" ]; then
            echo "Testing production URL: ${{ secrets.PRODUCTION_URL }}"
            curl -f "${{ secrets.PRODUCTION_URL }}/health" || echo "‚ö†Ô∏è Health check failed - normal if URL not configured"
            curl -f "${{ secrets.PRODUCTION_URL }}/api/v1/health" || echo "‚ö†Ô∏è API health failed - normal if URL not configured"
            
            # Verify SSL certificate for HTTPS URLs
            if [[ "${{ secrets.PRODUCTION_URL }}" == https://* ]]; then
              domain=$(echo "${{ secrets.PRODUCTION_URL }}" | sed 's|https://||' | sed 's|/.*||')
              openssl s_client -connect "$domain:443" -servername "$domain" < /dev/null || echo "‚ö†Ô∏è SSL check failed - normal if not configured"
            fi
          else
            echo "‚ö†Ô∏è PRODUCTION_URL secret not set - skipping smoke tests"
            echo "Add PRODUCTION_URL secret to enable automated health checks"
          fi

  # üîÑ STEP 4: Database Migrations (runs in parallel with deployments)
  database-migration:
    name: üîÑ Database Migration
    runs-on: ubuntu-latest
    # needs: [quality-check]  # Uncomment to require tests first
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/develop'  # Run for main/staging/develop
    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üì¶ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Install dependencies
        run: npm ci

      - name: üîÑ Run Prisma Migrations on Staging DB
        run: |
          echo "üîÑ Running database migrations on staging..."
          if [ -n "${{ secrets.DATABASE_URL_STAGING }}" ]; then
            npm run prisma:migrate  # Runs Prisma migrations
          else
            echo "‚ö†Ô∏è DATABASE_URL_STAGING secret not set - skipping database migrations"
            echo "Add DATABASE_URL_STAGING secret to enable migrations"
          fi
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_STAGING }}  # Point Prisma to staging DB

  # üßπ STEP 5: Cleanup (runs after deployments complete)
  cleanup:
    name: üßπ Cleanup
    runs-on: ubuntu-latest
    needs: [deploy-staging, deploy-production]  # Wait for both deployments
    if: always()  # Run even if deployments fail
    steps:
      - name: üßπ Clean up old Docker images from ECR
        run: |
          echo "üßπ Cleaning up old Docker images..."
          # TODO: Add ECR cleanup logic to remove old/unused images
          # Example: aws ecr batch-delete-image --repository-name smart-edu --image-ids imageTag=old-tag

      - name: üìä Deployment Summary
        run: |
          echo "üìä Deployment Summary:"
          echo "‚úÖ Docker images built and pushed to ECR"
          echo "‚úÖ Database migrations completed"
          echo "‚úÖ Application deployed to environments"
          echo "‚úÖ Smoke tests executed"
